---
title: "3.1 Tasks: Monitor Alertmanager"
weight: 2
sectionnumber: 3.1
---

{{% alert title="Note" color="primary" %}}
This setup is only suitable for our lab environment. In real life, you must consider how to monitor your monitoring infrastructure:
Having an Alertmanager instance as an Alertmanager AND as a target only in the same Prometheus is a bad idea!
{{% /alert %}}

### Task {{% param sectionnumber %}}.1: Add Alertmanager as monitoring target

This is repetition: Alertmanager also exposes metrics which can be scraped by Prometheus.

**Task description**: Configure the metric endpoint of Alertmanager in Prometheus and check, if the target in Prometheus can be scraped.

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Configure a new job under `scrape_configs` in `/etc/prometheus/prometheus.yml`:
```yaml
  - job_name: 'alertmanager'
    static_configs:
    - targets: ['localhost:9093']
```

Reload Prometheus
```bash
killall -HUP prometheus
```

Check in the [Prometheus web UI](http://LOCALHOST:9090/targets) if the target can be scraped.

{{% /details %}}

### Task {{% param sectionnumber %}}.2: Query an Alertmanager metric

After you add the Alertmanager metrics endpoint, you will have huge bunch of different values and identifiers.

**Task description**: Use curl to get a list of all available metrics and query any one from the Alertmanager.

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

To find out which metrics are available for one service you might query its metrics endpoint with `curl`, e.g. for Alertmanager:

```bash
curl localhost:9093/metrics
```

Then you get all metrics as follows (shortened), and you can pick whatever you're interested in.

```openmetrics
# HELP alertmanager_alerts How many alerts by state.
# TYPE alertmanager_alerts gauge
alertmanager_alerts{state="active"} 0
alertmanager_alerts{state="suppressed"} 0
# HELP alertmanager_alerts_invalid_total The total number of received alerts that were invalid.
# TYPE alertmanager_alerts_invalid_total counter
alertmanager_alerts_invalid_total{version="v1"} 0
alertmanager_alerts_invalid_total{version="v2"} 0
# HELP alertmanager_alerts_received_total The total number of received alerts.
# TYPE alertmanager_alerts_received_total counter
alertmanager_alerts_received_total{status="firing",version="v1"} 0
alertmanager_alerts_received_total{status="firing",version="v2"} 0
alertmanager_alerts_received_total{status="resolved",version="v1"} 0
alertmanager_alerts_received_total{status="resolved",version="v2"} 0
...
```

{{% /details %}}

### Task {{% param sectionnumber %}}.3: Get all Metrics from Alertmanager

**Task description**: Query all metrics which are exposed by Alertmanager using the Prometheus web UI.

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Execute a query in the Prometheus web UI without a metric name, but with the label `job="alertmanager"`:

```promql
{job="alertmanager"}
```

{{% /details %}}

### Task {{% param sectionnumber %}}.4 (optional): Last successful Alertmanager configuration reload

**Task description**: Use PromQL to answer the following question: How many minutes ago was the last successful configuration reload of Alertmanager?

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

```promql
(time() - alertmanager_config_last_reload_success_timestamp_seconds) / 60
```

* `time()` will return the current UNIX Epoch timestamp
* `alertmanager_config_last_reload_success_timestamp_seconds` will return the UNIX Epoch timetamp of the last successful Alertmanager configuration reload
* `/60` will calculate the result in minutes
