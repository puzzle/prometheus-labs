---
title: "3.3 Alertrules and alerts"
weight: 2
sectionnumber: 3
---

{{% alert title="Note" color="primary" %}}

To go on with the training we need a basic simulation app which exposes a metric endpoint. Let's install and start a sample application:

```bash
mkdir sample-app; cd sample-app
curl -L -O https://github.com/rotscher/prometheus-sample-app/releases/download/v0.0.6/prometheus-sample-app-v0.0.6-linux-amd64.tar.gz
tar fvxz prometheus-sample-app-v0.0.6-linux-amd64.tar.gz
./prometheus-sample-app server &
```

Finally, the target must be registered in Prometheus (don't forget to reload or restart Prometheus):

```yaml
  - job_name: 'sample-app'
    static_configs:
    - targets: ['localhost:8080']
```

{{% /alert %}}

### Task 1

* Define an alert rule which sends an alert when a target is down
* Wait time until the alarm should be sent is 2 minutes
* Give the necessary information about which instance and job is down

### Task 2

* Kill or stop the sample application
* What do you observe in Prometheus UI or in Alertmanager UI?

### Task 3

* Check with the help of a metric if there are failures in Alertmanager.

## Solutions

{{% details title="Task 1" %}}

Create a new file `alertrule.yml` and add the following snippet:

```yaml
groups:
- name: basic
  rules:
  - alert: Up
    expr: up == 0
    for: 2m
    labels:
      severity: page
    annotations:
      summary: Instance {{ $labels.instance }} of job {{ $labels.job }} is down
```

In `prometheus.yml` add the rule file at `rule_files` section and restart or reload Prometheus.

```yaml
  rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"
  - alertrule.yml
```
{{% /details %}}

{{% details title="Task 2" %}}

* In the Prometheus UI there is an [alert console](http://localhost:9090/alerts) which shows you information about inactive and active alerts.
* As soon as an alert is in state `FIRING` the alert is sent to Alertmanager. You should see the alert in its [webinterface](http://localhost:9093/).

{{% /details %}}

{{% details title="Task 3" %}}

As alertmanager is monitored by Prometheus, start typing `alertmanager_fail` and
look for a metric which could give you the information you're looking for.

```
alertmanager_notifications_failed_total
```

We expect this value to be 0 (or stable)

{{% /details %}}
