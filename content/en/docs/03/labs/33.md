---
title: "3.3 Tasks: Alertrules and alerts"
weight: 2
sectionnumber: 3.3
---

{{% alert title="Note" color="primary" %}}

For doing the alerting lab it's useful to have a "real" application so that alerts can be provoked. The training app installed in the previous lab provides a sample app; you can start it as follows:

```bash
cd ~/work/prometheus-training-app-v0.0.2-linux-amd64
./prometheus-training-app sampleapp &
```

The example app exposes metrics at `http://localhost:8080/metrics`

Also, the target must be registered in the Prometheus config (`/etc/prometheus/prometheus.yml`) (don't forget to reload or restart Prometheus):

```yaml
  - job_name: 'sample-app'
    static_configs:
    - targets: ['localhost:8080']
```

{{% /alert %}}

### Task {{% param sectionnumber %}}.1: Configure a target down alert

Refer to the [official documentation](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) to see which fields you can specify for an alerting rule.

**Task description:**

* Define an alerting rule which sends an alert when a target is down. Remember the `up` metric?
* The wait time until the alarm should be sent is 2 minutes
* Add a label `team` with the value `team-b`
* Add an annotation `summary` with information about which instance and job is down

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

Create a new file `alertrule.yml` for Prometheus in `/etc/prometheus/` and add the following snippet:

```yaml
groups:
- name: basic
  rules:
  - alert: Up
    expr: up == 0
    for: 2m
    labels:
      team: team-b
    annotations:
      summary: Instance {{ $labels.instance }} of job {{ $labels.job }} is down
```

The value in field `for` is the wait time until the active alert gets in state `FIRING`. Before that, the alert is `PENDING` and not yet sent to Alertmanager.

The alert is instrumented with the labels from the metric (e.g. `job`and `instance`). Additional labels can be defined in the rule. Labels can be used in Alertmanager for the routing.

With annotations, additional human-readable information can be attached to the alert.

In `/etc/prometheus/prometheus.yml`, add the rule file at `rule_files` section and restart or reload Prometheus.

```yaml
  rule_files:
  ...
  - "alertrule.yml"
```

* In the Prometheus web UI there is an **Alerts** [menu item](http://LOCALHOST:9090/alerts) which shows you information about the alerts.

{{% /details %}}

### Task {{% param sectionnumber %}}.2: Verify the target down alert

**Task description:**

* Kill or stop the sample application
* Verify that the sample app no longer exposes metrics
* What do you observe in Prometheus UI or in Alertmanager UI?

{{% details title="Hints" mode-switcher="normalexpertmode" %}}

* Kill the example app and make sure it will no longer exposes metrics

```bash
pkill -f "prometheus-training-app sampleapp"
curl http://localhost:8080/metrics
```

```bash
curl: (7) Failed connect to localhost:8080; Connection refused
```

* The Prometheus web UI **Alerts** [menu item](http://LOCALHOST:9090/alerts) shows you information about inactive and active alerts.
* As soon as an alert is in state `FIRING` the alert is sent to Alertmanager. You should see the alert in its [web UI](http://LOCALHOST:9093).
* You can also check the mail in [mailcatcher](http://LOCALHOST:1080)

{{% /details %}}

### Task {{% param sectionnumber %}}.3: Identify the notified receivers

**Task description**: Who is finally notified?

{{% details title="Hints" mode-switcher="normalexpertmode" %}}
Receivers `receiver-a` and `receiver-b` should be notified in this case.

Explanation:

* The label `team: team-b` set on the alert rule matches both receivers
* `continue: true` is set to true, therefore both receivers will be notified

```yaml
...
  routes:
    - receiver: 'receiver-a'
      match:
        team: 'team-a'
      continue: true
    - receiver: 'receiver-b'
      match_re:
        team: 'team-[a|b]'
...
```

