---
title: "7.2 Tasks: kube-prometheus setup"
weight: 1
sectionnumber: 7
---
The [kube-prometheus](https://github.com/prometheus-operator/kube-prometheus) stack already provides an extensive Prometheus setup and contains a set of default alerts and dashboards from [Prometheus Monitoring Mixin for Kubernetes](https://github.com/kubernetes-monitoring/kubernetes-mixin). The following targets will be available.

**kube-state-metrics:** Exposes metadata information about Kubernetes resources. Used, for example, to check if resources have the expected state (deployment rollouts, pods CrashLooping) or if jobs fail. Example metrics:

```promql
kube_deployment_created
kube_deployment_spec_replicas
kube_daemonset_status_number_misscheduled
...
```

**kubelet/cAdvisor:** [Advisor](https://github.com/google/cadvisor) exposes usage and performance metrics about running container. Commonly used to obseve memory usage or [CPU throttling](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/). Example metrics:

```promql
container_cpu_cfs_throttled_periods_total
container_memory_working_set_bytes
container_fs_inodes_free
...
```

**kubelet:** Exposes general kubelet related metrics. Used to observe if the kubelet and the container engine is healthy. Example metrics:

```promql
kubelet_docker_operations_duration_seconds_bucket
kubelet_runtime_operations_total
...
```

**apiserver:** Metrics from the Kubernetes API server. Commonly used to catch errors on resources or problems with latency. Example metrics:

```promql
apiserver_request_duration_seconds_bucket
apiserver_request_total{code="200",...}
...
```

**kubelet/probes:** Expose metrics about [Kubernetes liveness, readiness and startup probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) Normally you would not alert on Kubernetes probe metrics, but on container restarts exposed by `kube-state-metrics`. Example metrics:

```promql
prober_probe_total{probe_type="Liveness", result="successful",...}
prober_probe_total{probe_type="Startup", result="successful",...}
...
```

**blackbox-exporter:** Exposes default metrics from blackbox-exporter. Can be customized using the [Probe](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#probe) custom resource. Example metrics:

```promql
probe_http_status_code
probe_http_duration_seconds
...
```

**node-exporter:** Exposes the hardware and OS metrics from the nodes running Kubernetes. Example metrics:

```promql
node_filesystem_avail_bytes
node_disk_io_now
...
```

**alertmanager-main/grafana/prometheus-k8s/prometheus-operator/prometheus-adapter:** Exposes all monitoring stack component metrics. Example metrics:

```promql
alertmanager_alerts_received_total
alertmanager_config_last_reload_successful
...
grafana_alerting_active_alerts
grafana_datasource_request_total
...
prometheus_config_last_reload_successful
prometheus_rule_evaluation_duration_seconds
...
prometheus_operator_reconcile_operations_total
prometheus_operator_managed_resources
...
```

### Task 1

* Navigate to the Prometheus user interface
* Take a look at the provided default Prometheus rules
* Investigate if there is a default Alerting rule configured to monitor container restarts
* Which exporter exposes the required metrics?

{{% alert title="Hint" color="primary" %}}
Search for an alert with `CrashLooping` in its name
{{% /alert %}}

### Task 2

* Display the memory usage of both Prometheus Pods
* Filter to display just the `prometheus` container

{{% alert title="Hint" color="primary" %}}
Search for an metric with `memory_working_set` in its name
{{% /alert %}}

### Task 3

* Display how many pods are currently running on your Kubernetes platform

### Task 4

* Add a custom Prometheus rule to the monitoring stack
* Check if you have reached the defined Retention size
* Set the labels `prometheus: k8s` and `role: alert-rules` on your PrometheusRule to match the resource with your Prometheus

See Prometheus custom resource as reference

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
...
spec:
  ruleSelector:
    matchLabels:
    prometheus: k8s
    role: alert-rules
...
```

{{% alert title="Hint 1" color="primary" %}}
Use the [PrometheusRule](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusrule) custom resource to define Prometheus rules. You can take a look at existing PrometheusRules or at this [example](https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/alerting.md#fire-alerts) about the format of PrometheusRules.
{{% /alert %}}

{{% alert title="Hint 2" color="primary" %}}
You can use the `prometheus_tsdb_size_retentions_total` metric
{{% /alert %}}

## Solutions

{{% details title="Task 1" %}}

The Alerting rule is called `KubePodCrashLooping`

The PromQL defined for alerting is

```promql
rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[10m]) * 60 * 5 > 0
```

When you take a look at the query you will see, that there is filter to use just metrics from the `kube-state-metrics` exporter

{{% /details %}}

{{% details title="Task 2" %}}

```promql
container_memory_working_set_bytes{pod=~"prometheus-k8s-.*", container="prometheus"}
```

{{% alert title="Hint" color="primary" %}}
Your query returns 4 time series with the same value but different labels. One has as additional information and allows you to filter the Docker id. E.g.

```promql
container_memory_working_set_bytes{id="/docker/34750eee3d37c8cd3594fc46bb20ed166374ece7737c590b81ed1847aaa21d50/kubepods/burstable/pode2ab8dc5-ad8e-4b5c-9b0f-49c3bb5a8a34/dce2373557fb05e0bc9819b9f786f6e3bcc882280ee2eb9267edb7040886a55b",pod="prometheus-k8s-0", container="prometheus", ...}
container_memory_working_set_bytes{id="/kubepods/burstable/pode2ab8dc5-ad8e-4b5c-9b0f-49c3bb5a8a34/dce2373557fb05e0bc9819b9f786f6e3bcc882280ee2eb9267edb7040886a55b", pod="prometheus-k8s-0", container="prometheus",  ...}
```

{{% /alert %}}

{{% /details %}}

{{% details title="Task 3" %}}

There are different ways to archive this. You can for example query all running containers and group them by `pod` and `namespace`.

```promql
count(sum(kube_pod_container_status_running) by (pod,namespace))
```

You may also sum() all running pods on your Kubernetes nodes

```promql
sum(kubelet_running_pods)
```

{{% /details %}}

{{% details title="Task 4" %}}

There are different ways to archive this. The first approach is to just check the number of times that blocks were deleted because the maximum number of bytes was exceeded.

```yaml
cat <<EOF > ~/work/prometheus-custom-rule.yml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-rule
  namespace: monitoring
  labels:
    prometheus: k8s
    role: alert-rules
spec:
  groups:
    - name: custom.rules
      rules:
        - alert: PrometheusReachedRetentionSize
          annotations:
            summary: "Prometheus deleted blocks because the maximum number of bytes was exceeded in {{ $labels.namespace }} {{ $labels.pod }}"
          expr: rate(prometheus_tsdb_size_retentions_total[5m]) > 0
          labels:
            severity: warning
EOF
kubectl -n monitoring create -f ~/work/prometheus-custom-rule.yml
```

Another approach is to alert, when the on-disk time series database size is greater than `prometheus_tsdb_retention_limit_bytes`. For example:

```promql
prometheus_tsdb_storage_blocks_bytes >= prometheus_tsdb_retention_limit_bytes
```

{{% /details %}}
